# ./chatbot/rag/QA_Cohere_Handler.py

import os
import re
import random
import logging
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_cohere.chat_models import ChatCohere
from chatbot.rag.utils.singleton_meta import SingletonMeta
from chatbot.rag.handlers.base_handler import BaseQAHandler
from chatbot.rag.utils.patterns import (
    prompt_template,
    greetings,
    greeting_messages,
    farewell,
    farewell_messages,
    gratefulness,
    gratefulness_messages,
)
from websearch.search import search_web

load_dotenv()

logger = logging.getLogger(__name__)

class QA_CohereHandler(BaseQAHandler, metaclass=SingletonMeta):
    """
    Handler to manage interactions with the Cohere model for generating responses
    based exclusively on web search results using Tavily.
    """
    
    def __init__(self, model: str, temperature: float, max_tokens: int):
        """
        Initializes the handler with model parameters and prompt template.
        Now exclusively uses web search for context retrieval.

        Args:
            model (str): The type of Cohere model.
            temperature (float): Level of randomness for response generation.
            max_tokens (int): Maximum number of tokens in the generated response.
        """
        # Avoid multiple initializations
        if hasattr(self, '_initialized') and self._initialized:
            return
        self._initialized = True

        # Model parameter configuration
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        logger.info(f'Temperature: {temperature}')
        logger.info(f'Max Tokens: {max_tokens}')

        # Load prompt template and LLM
        self.load_prompt_template()
        self.load_llm()
        
        logger.info('Cohere Handler creado correctamente (solo búsqueda web).')
        
    def load_prompt_template(self):
        """
        Loads the prompt template for generating queries.
        """
        try:
            self.prompt = PromptTemplate(
                template = prompt_template,
                input_variables = ["context", "question"]
            )
            logger.info('PromptTemplate cargado correctamente.')
        except Exception as e:
            logger.error('Ha ocurrido un error al cargar el PromptTemplate.', exc_info=True)

    def load_llm(self):
        """
        Loads the Cohere LLM model to generate responses.
        """
        try:
            self.llm = ChatCohere(
                model=self.model,
                temperature=self.temperature,
                cohere_api_key=os.getenv('COHERE_API_KEY'),
                max_tokens=self.max_tokens
            )
            logger.info('LLM cargado correctamente.')
        except Exception as e:
            logger.error(f'Ha ocurrido un error al cargar el LLM {self.model}.', exc_info=True)

    def get_web_context(self, web_results: list) -> str:
        """
        Optimiza el contexto web combinando múltiples resultados de manera inteligente.
        
        Args:
            web_results (list): Lista de resultados de búsqueda web
            
        Returns:
            str: Contexto web optimizado para el prompt
        """
        if not web_results:
            return ""
        
        context_parts = []
        total_length = 0
        max_context_length = 4000  # Límite dinámico basado en max_tokens
        
        for i, result in enumerate(web_results):
            # Preferir raw_content sobre content
            content = result.get('raw_content', result.get('content', ''))
            
            if content:
                # Agregar metadatos útiles
                source_info = f"[Fuente {i+1}: {result.get('title', 'Sin título')} - {result.get('url', '')}]"
                formatted_content = f"{source_info}\n{content}\n"
                
                # Control inteligente de longitud
                if total_length + len(formatted_content) <= max_context_length:
                    context_parts.append(formatted_content)
                    total_length += len(formatted_content)
                else:
                    # Incluir parcialmente si queda espacio
                    remaining_space = max_context_length - total_length - len(source_info) - 20
                    if remaining_space > 100:  # Solo si vale la pena
                        truncated_content = content[:remaining_space] + "..."
                        context_parts.append(f"{source_info}\n{truncated_content}\n")
                    break
        
        return "\n".join(context_parts)

    def get_answer(self, query: str) -> str:
        """
        Generates an answer for the given query using the Cohere model with web search context.

        Args:
            query (str): The user's query or question.

        Returns:
            str: The response generated by the Cohere model.
        """
        try:
            # Verificar patrones predefinidos
            if any(re.match(pattern, query.lower()) for pattern in greetings):
                return random.choice(greeting_messages)
            elif any(re.match(pattern, query.lower()) for pattern in farewell):
                return random.choice(farewell_messages)
            elif any(re.match(pattern, query.lower()) for pattern in gratefulness):
                return random.choice(gratefulness_messages)
            
            # Búsqueda web optimizada
            logger.info(f"Realizando búsqueda web para: '{query}'")
            web_results = search_web(query)
            
            if not web_results:
                logger.warning(f"No se encontraron resultados web para: '{query}'")
                return "Lo siento, no pude encontrar información relevante en la web para responder tu consulta."
            
            # Generar respuesta usando el prompt optimizado
            formatted_prompt = self.prompt.format(context=web_results, question=query)
            
            logger.info(f"Generando respuesta con contexto de {len(web_results)} fuente(s)")
            response = self.llm.invoke(formatted_prompt).content
            
            return response
            
        except Exception as e:
            logger.error('Ha ocurrido un error en la ejecución del Query.', exc_info=True)
            return "Lo siento, ha ocurrido un error al procesar tu consulta."
