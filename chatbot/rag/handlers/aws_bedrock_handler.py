# ./chatbot/rag/QA_AWS_Bedrock_Handler.py

import re
import random
import logging
from langchain_core.prompts import PromptTemplate
from chatbot.rag.utils.singleton_meta import SingletonMeta
from chatbot.rag.handlers.base_handler import BaseQAHandler
from chatbot.rag.utils import utils
from ..clients.aws_client import get_client
from chatbot.rag.utils.patterns import (
    prompt_template,
    greetings,
    greeting_messages,
    farewell,
    farewell_messages,
    gratefulness,
    gratefulness_messages,
)

logger = logging.getLogger(__name__)

class QA_AwsBedrockHandler(BaseQAHandler, metaclass=SingletonMeta):
    """
    Singleton class to handle interactions with the AWS Bedrock model
    for generating responses based on documents retrieved using TF-IDF.
    """

    def __init__(self, model: str, temperature: float, max_tokens: int, docs_directory: str,
                 chunk_size: int = 500, chunk_overlap: int = 0):
        """
        Initializes the handler with model parameters, prompt template, and document database.

        Args:
            model (str): The model ID for the AWS Bedrock model.
            temperature (float): Level of randomness for response generation.
            max_tokens (int): Maximum number of tokens in the generated response.
            docs_directory (str): Directory path to the documents database.
            chunk_size (int): Size of the chunks when splitting documents for retrieval.
            chunk_overlap (int): Size of the overlap between document chunks.
        """
        # Avoid multiple initializations
        if hasattr(self, '_initialized') and self._initialized:
            return
        self._initialized = True

        # Model parameter configuration
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        logger.info(f'Temperature: {temperature}')
        logger.info(f'Max Tokens: {max_tokens}')

        # Load prompt template, documents, and AWS client
        self.load_prompt_template()
        self.tfidf_retriever = utils.load_documents_database(docs_directory, chunk_size, chunk_overlap)
        self.aws_client = get_client()
        
        logger.info('AWS Bedrock Handler creado correctamente.')

    def load_prompt_template(self):
        """
        Loads the prompt template for generating queries.
        """
        try:
            self.prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )
            logger.info('PromptTemplate cargado correctamente.')
        except Exception as e:
            logger.error('Ha ocurrido un error al cargar el PromptTemplate.', exc_info=True)

    def get_context(self, query: str) -> str:
        """
        Retrieves the most relevant context from the document database for a given query.

        Args:
            query (str): The user's input or question that needs context.

        Returns:
            str: A concatenated string of the content of the most relevant documents retrieved.
        """
        retrieved_docs = self.tfidf_retriever.invoke(query)
        context = " ".join(doc.page_content for doc in retrieved_docs)
        return context
        
    def get_answer(self, query: str) -> str:
        """
        Generates an answer for the given query using the AWS Bedrock model.

        Args:
            query (str): The user's query or question.

        Returns:
            str: The response generated by the AWS Bedrock model.
        """
        try:
            if any(re.match(pattern, query.lower()) for pattern in greetings):
                response_text =  random.choice(greeting_messages)
            elif any(re.match(pattern, query.lower()) for pattern in farewell):
                response_text = random.choice(farewell_messages)
            elif any(re.match(pattern, query.lower()) for pattern in gratefulness):
                response_text = random.choice(gratefulness_messages)
            else:
                # Get Context
                context = self.get_context(query)
                
                # Build the conversation structure for the AWS Bedrock API
                conversation = [
                    {
                        "role": "user",
                        "content": [{"text": self.prompt_template.format(
                            context=context,
                            question=query
                        )}]
                    }
                ]
                
                # Call the AWS Bedrock model to get the response
                response = self.aws_client.converse(
                    modelId=self.model,
                    messages=conversation,
                    inferenceConfig={
                        "maxTokens": self.max_tokens,
                        "temperature": self.temperature
                    },
                    additionalModelRequestFields={"k": 0}
                )

                # Extract and return the response generated by the model
                response_text = response["output"]["message"]["content"][0]["text"]
            return response_text

        except Exception as e:
            logger.error('Ha ocurrido un error al realizar la pregunta.', exc_info=True)
            return "Lo siento, ha ocurrido un error al procesar tu pregunta."
